
## 学生学习问题

### 1 有终止状态的马尔可夫链

- 在租车问题中，车辆不断地在四个门店中按一定概率随机出现，没有尽头（除非车辆损坏更换或者公司倒闭）。
- 在醉汉回家问题中，只有左右两个状态可以选择（当然也可原地发呆），所以醉汉最终是可以到家的，并且不再出来，是一个有结束条件的过程。

下面我们要结合上面两个问题的特点，来学习更复杂的情况：
1. 状态更多
2. 转移更复杂
3. 有终止状态

当然，真实世界的马尔科夫链的状态空间很可能是成千上万个，转移路径也更复杂。

### 2 学生学习问题

图 1 是一个状态转移图，也可以叫做马尔可夫链。

<img src="./img/Student-1.png" width="500">

图 1 学习问题的状态转移概率图

- Class 1,2,3
  上课/学习/复习状态，假设一门课是需要三次课的学习就可以结束。
  - 在课程 1 中，有 0.5 的概率跑到 Game 状态，在课堂上用手机偷偷摸摸打游戏，另外 0.5 的概率到课程2状态。
  - 在课程 2 中，有 0.2 的概率直接退课，另外 0.8 的概率到课程3状态。
  - 在课程 3 中，有 0.6 的概率去考试，另外 0.4 的概率以为考试还远，不着急准备呢，就去休息了。
- Pass
  考试通过状态。考试结束后，以 100% 概率到结课状态。
- Rest
  休息状态。休息结束后，发现学的内容都忘得差不多了，遂分别以不同的概率回到三次课的学习/复习状态。
- Game
  娱乐/打游戏状态。在游戏状态中很大可能不能自拔，以 0.9 概率继续打游戏，只有 0.1 的概率幡然悔悟回到学习状态。
- End
  结课/退课状态。进入此状态后将不再进行转移，或者是说以 100% 的概率转移到自己，叫做结束状态或者吸收状态。

表 1 中列出了状态转移矩阵，与租车问题中的相同。

表 1

|P: 从$\rightarrow$到|Class1|Class2|Class3|Pass|Rest|Game|End|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**Class1**||0.5||||0.5||
|**Class2**|||0.8||||0.2|
|**Class3**||||0.6|0.4|||
|**Pass**|||||||1.0|
|**Rest**|0.2|0.4|0.4|||||
|**Game**|0.1|||||0.9|||
|**End**|||||||1.0|

有的读者可能有个疑问：打游戏上瘾，从 Game 到 Game 有 0.9 的高概率，那么当该学生打游戏 2 小时后良心发现，转到学习状态的概率会不会大于 0.1 呢？

这是一个简单的平稳环境的马尔科夫链，如果考虑更复杂的情况，可以在 Game 状态下增加一个计数器：
- 如果打游戏超过 1 小时了，则有 0.5 的概率回到学习状态；
- 如果没超过 1 小时，则有 0.1 的概率回到学习状态。

### 3 分幕和采样

由于终止状态的存在（图 1 中的 End 状态），可以引入一个新的强化学习中的重要概念：**分幕**（Episode）。比如：

- 在醉汉回家问题中，醉汉到家了，整个过程叫做一幕；第二天该醉汉又喝醉了，再次到家后，又叫做一幕。
- 在学生学习问题中，学生经过一系列折腾，最终到达 End 状态，叫做一幕。
- 一盘棋中，最终双方分出输赢或者打平，叫做一幕。
- 打桥牌中，每个人出完手中的最后一张牌，桌面上有 13 墩牌，叫做一幕。
- 一个扫地机器人，完成当天任务回到充电状态，叫做一幕；如果在打扫过程中电量过低，不得不暂时回到充电状态，不叫作完成一幕。

在图 1 中，可以根据不同的学生在本门课中的经历，获得不同的到达终点路径，其中，用 C1,C2,C3 表示 Class 1,Class 2,Class 3。比如：
- C1（表示Class 1，下同）- C2 - C3 - Pass - End
- C1 - Game - Game - Game - C1 - C2 - End
- C1 - C2 - C3 - Rest - C2 - C3 - Pass - End
- C1 - C2 - C3 - Rest - C1 - Game - Game - C1 - C2 - End
......

上述的过程叫做**采样**（Sample）。对于大多数学生来说，经历的都是第一个路径，而有极少数学生走的第四个路径。

根据转移概率，经过采样后到达终点，得到一幕状态序列。没有到达终点的状态序列不叫作完整的状态序列，也不叫做一幕，但是这些序列片段仍然有研究价值。

另外，读者可能注意到在上述采样的例子中，都是从 C1 开始的，似乎有一个隐含的开始状态。在某些问题中，我们指定一些状态为开始状态，原因是：

- 一是为了符合实际问题的逻辑需要，比如学生学习问题，一般都是从第一节课开始的。
- 二是为了不过分简化问题的难度。比如一个迷宫，如果不指定开始状态，而是从快到出口的一个位置作为起始位置，将会大大降低问题的难度。

### 4 奖励和收益

从图 1 中，读者可以体会到，在 C3 状态时，会比在 C1 状态有更多的可能性到达考试通过并结课的状态的（不考虑退课的情况），那么 C3 状态比 C1 状态要“好”。如何定义这个“好”呢？这就要引入强化学习的两个重要概念：**奖励**或者**收益**（Reward），以及**回报**（Return 或 Gain）。

#### 奖励

在状态转移图中，给每个状态都定义一个奖励值，当到达这个状态时，强化学习过程就会获得相应的奖励值，使得整个过程向着获得最大收益的方向优化和运行。

在强化学习中，智能体的目标被形式化表征为一种特殊信号，称为收益，它通过环境传递给智能体。在每个时刻，收益都是一个单一标量数值。非正式地说，智能体的目标是最大化其收到的总收益。这意味着需要最大化的不是当前收益，而是长期的累积收益。我们可以将这种非正式想法清楚地表述为收益假设：
我们所有的“目标”或“目的”都可以归结为:最大化智能体接收到的标量信号(称之为收益)累积和的概率期望值。

使用收益信号来形式化目标是强化学习最显著的特征之一。

智能体总是学习如何最大化收益。如果我们想要它为我们做某件事，我们提供收益的方式必须要使得智能体在最大化收益的同时也实现我们的目标。因此，至关重要的一点就是，我们设立收益的方式要能真正表明我们的目标。特别地，收益信号并不是传授智能体如何实现目标的先验知识。例如、国际象棋智能体只有当最终获胜时才能获得收益，而并非达到某个子目标，比如吃掉对方的子或者控制中心区域。如果实现这些子目标也能得到收益，那么智能体可能会找到某种即使绕开最终目的也能实现这些子目标的方式。例如它可能会找到一种以输掉比赛为代价的方式来吃对方的子。收益信号只能用来传达什么是你想要实现的目标，而不是如何实现現这个目标。



#### 回报

在英文中用 Return 来表示，但是在公式中一般用 G 来表示，所以我们不妨理解为 Gain 这个单词。而且 Return 的首字母与奖励（Reward）的首字母相同，容易混淆。


$$
G_t = R_{t+1}+R_{t+2}+R_{t+3}+ \cdots +R_{T} \tag{1}
$$

### 马尔可夫奖励过程（Markov Reward Process）


### 参考资料

- David Silver RL course
- Sutton 强化学习